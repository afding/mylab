
%!Target : html
%!encoding: utf-8
%!options: --toc --toc-level 2 --css-sugar

+功能+
给中文文本加英文索引。之前看冰与火之歌的小说，人名很多，发现看英文名更方便。于是做了这个web应用。可以修改/编辑索引。从本机选某个中文txt上传。就可以看到英文修饰的了。

最初发布在豆瓣小组中:
http://www.douban.com/group/topic/32220888/

目前可用的中英文索引表，只有《冰与火之歌》，主要是从网上搜集然后手工整理，内容在：
[worddict ""/worddict""]

+技术+
用nginx+pyramid搭建的web server。 deploy在免费的AWS-EC2上，硬件性能不高。

-web前端借用jqueryUI；
- 为方便交换，将操作放在同一页面；用ajax上传/返回文字内容；
- 不提高下载，避免成为web存储，也规避了版权问题。
- 因为user较少，为方便，没有使用sign in方法，用户共享一份索引表。当人数增大，将采用OAuth方法，目前已有开源模块可用：velruse（http://packages.python.org/velruse/）。

已尝试网站：
[login ""/login""]


+自动生成人名对照+
根据输入的中文/英文文本，自动生成对照索引。目前代码实现的正确率略高于80%；
performance约为15min，笔记本电脑运行，对于一册8万字中文文本。 其中生成词库约半分钟。大部分时间在中英文词匹配， 100英文词*2k中文词。

某册《冰与火之歌》的运行结果见：
[worddict.res.txt ""/static/worddict.res.txt""]

格式：
英文名：频率， 中文名：distance：距离（误差）

e.g.:
Aegon:57, 伊耿:distance:130

因为比较耗时，所以没有开放成API。

主要方法是先对中文分词，再使用一元线性回归，对可能的中英文词匹配，求最优值。对一些关联词做了附加处理，比如复数形式，字串。
因为多数人命是不非常用词，用固定词库的分词工具无法区分。这里用自己之前编写的自动生成词库的模块。

误差的主要问题是有些名字的翻译不是严格对应，中英文词的列表长度不等。目前是简单忽略掉尾部多出的词，而线性回归对错位造成的误差敏感；如果尝试忽略所有可能的位置，计算量较大，所以没有采用。


